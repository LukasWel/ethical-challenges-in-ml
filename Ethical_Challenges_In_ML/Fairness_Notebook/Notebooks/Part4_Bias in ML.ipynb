{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5400bbc6-f28b-4d03-adb3-2abbcfabc884",
   "metadata": {},
   "source": [
    "# Part 4: Bias in Machine Learning\n",
    "\n",
    "### What is Bias?\n",
    "\n",
    "Bias refers to systematic **distortions** that lead to **unfair outcomes**. In machine learning, bias often arises **unintentionally** and is closely tied to **discrimination**. Bias can occur at any stage of the ML pipeline and is often introduced through **training data, modeling decisions, or deployment context**.\n",
    "\n",
    "Bias is not always harmful. However, when it influences decisions about people’s lives, we must understand and address it. **Recognizing** and **reducing harmful bias is key** to building fair and trustworthy systems.<sup>1,</sup><sup>2</sup>\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Bias in Machine Learning\n",
    "\n",
    "There are many forms of bias that can affect data-driven systems. This notebook focuses on the seven bias categories introduced by **Suresh & Guttag (2021)**, which are especially relevant in the **machine learning lifecycle**. Each category highlights a specific entry point for bias and includes relevant subtypes.\n",
    "\n",
    "### 1. Historical Bias<sup>3</sup>\n",
    "Bias that already exists in the world, **before** any data collection, model training, or algorithmic decision-making takes place. It reflects **structural inequalities and social patterns** that are encoded in the data.\n",
    "\n",
    "**Note:** According to Suresh & Guttag (2021), historical bias is the most fundamental form of bias. It includes **many other biases caused by users or society**. The subtypes below are just a small selection of common examples.\n",
    "\n",
    "- **Subtypes:**<sup>1</sup>  \n",
    "  - *Temporal bias*: outdated data that does not reflect current realities  \n",
    "  - *Content production bias*: some groups produce more or different data (e.g. online content)  \n",
    "  - *Behavioral bias*: different behavior across platforms or contexts  \n",
    "  - *Social bias*: others’ behavior influences personal input (e.g. ratings)  \n",
    "  - *Self-selection bias*: participation in data generation is non-random  \n",
    "  - *User interaction bias*: feedback loops reinforce earlier behaviors\n",
    "\n",
    "- **Example:** Word embeddings link \"doctor\" to male and \"nurse\" to female, reflecting societal stereotypes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Representation Bias<sup>3</sup>\n",
    "Occurs when the data underrepresents parts of the target population, leading to models that generalize poorly for these groups.\n",
    "\n",
    "- **Subtypes:**  \n",
    "  - *Population bias*: target population is misdefined (e.g. based on outdated census data)  \n",
    "  - *Sampling bias*: data collection fails to reflect diversity (e.g. only hospital patients)  \n",
    "  - *Coverage bias*: not all subgroups are equally included  \n",
    "  - *Subset bias*: small groups (e.g. pregnant women) are statistically drowned out\n",
    "\n",
    "\n",
    "- **Example:** ImageNet contains mostly Western-centric images (45% from the U.S. vs. 1% from China), leading to reduced performance for underrepresented regions. → Problematic when a skewed dataset is used for model training\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Measurement Bias<sup>3</sup>\n",
    "Bias in how features or labels are defined, collected, or measured. It often stems from inaccurate proxies or unequal label quality across groups.\n",
    "\n",
    "- **Subtypes:**<sup>1,</sup><sup>4</sup>\n",
    "  - *Label bias*: labels don't reflect ground truth equally (e.g. arrest = crime?)  \n",
    "  - *Omitted variable bias*: important explanatory variables are missing  \n",
    "  - *Instrument bias*: the measurement tool itself performs differently across groups\n",
    "\n",
    "- **Example:** Using arrest records as a crime proxy leads to inflated risk scores for overpoliced communities. → Higher false positive rates for Black defendants in COMPAS\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Aggregation Bias<sup>3</sup>\n",
    "Happens when a single model is used across diverse subpopulations and uniform behavior is assumed.\n",
    "\n",
    "- **Subtype:**<sup>1</sup> \n",
    "  - *Simpson’s Paradox*: trends reverse or disappear when data is aggregated\n",
    "\n",
    "- **Example:** A model trained on general data misclassifies medical conditions in women because the average values are male-dominated.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Learning Bias<sup>3</sup>\n",
    "Bias introduced through modeling decisions, such as optimizing only for overall accuracy.\n",
    "\n",
    "- **Example:** A model may ignore underrepresented group patterns because they're harder to learn or contribute little to global accuracy.\n",
    "\n",
    "- **Note:** This often interacts with representation or measurement bias.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Evaluation Bias<sup>3</sup>\n",
    "Bias introduced when model evaluation uses benchmarks or metrics that do not reflect the real-world deployment population.\n",
    "\n",
    "- **Example:** Gender classification performs worst for dark-skinned women due to underrepresentation in benchmark datasets.<sup>5</sup>\n",
    "\n",
    "- **Impact:** Poor subgroup performance may remain unnoticed if metrics like overall accuracy are used.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Deployment Bias<sup>3</sup>\n",
    "When a model is applied in a context that differs from its training or evaluation phase — often without appropriate human oversight.\n",
    "\n",
    "- **Example:** A recidivism prediction model is used to determine prison sentences, but it was designed only to assess risk.\n",
    "\n",
    "- **Risk:** Even well-performing models can cause harm if deployed carelessly.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:**\n",
    "These bias types are **not isolated**. They often interact and reinforce each other in **feedback loops**. These dynamics will be discussed on page 6 of this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Exercise: Localize Forms of Bias in ML-Lifecycle\n",
    "\n",
    "Below is a simplified version of a figure from Suresh & Guttag (2021). The bias names have been replaced with numbers.\n",
    "\n",
    "> **Task:** Match each number to the correct bias type. When you are done you can check your answers in the user guide.\n",
    "\n",
    "![](Images/Suresh_Reverse.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Bias Amplification<sup>6</sup>\n",
    "\n",
    "Bias amplification happens when the model not only reflects existing bias in the data but **intensifies** it — producing more skewed predictions than expected. Research by Hall et al. (2022) identifies several key conditions under which amplification is most likely to occur:\n",
    "\n",
    "#### Strong Group Signals\n",
    "\n",
    "A **group signal** refers to information in the data that reveals **group membership** (e.g. gender or ethnicity) even if this feature is not explicitly included.\n",
    "\n",
    "- If the **group signal is strong** and easier to learn than the actual label,  \n",
    "  the model may rely on **group-related shortcuts** instead of task-relevant patterns.\n",
    "- This leads to **overgeneralization** and **amplified disparities** in predictions.\n",
    "- **Example:** If female students in the data graduate more often, and gender can be inferred from features like *field of study* or *high school GPA*, the model may overuse this information. This results in more graduation predictions just because the student appears to be female.\n",
    ">**Training dynamics:** Bias amplification is often highest at the beginning of training when models rely on simple group cues. It may decrease mid-training as class-specific patterns emerge, and slightly rise again in late phases.\n",
    "\n",
    "#### Model Capacity (V-Shaped Effect)\n",
    "\n",
    "**Model capacity** describes a model’s ability to learn complex patterns. It is influenced by architecture, depth, regularization, and other hyperparameters.\n",
    "\n",
    "- **Low-capacity models** underfit and rely on simple features — often group-related ones.\n",
    "- **High-capacity models** overfit, learning spurious or biased correlations.\n",
    "- Amplification is highest at both extremes → the **relationship follows a V-shape**.\n",
    "- Careful tuning and regularization (e.g. weight decay) can help reduce this effect — but often require trading off accuracy.\n",
    "\n",
    "#### Data Size and Bias\n",
    "\n",
    "The structure and size of the training data matter:\n",
    "\n",
    "- **Small datasets** can lead to overfitting or reliance on noisy group cues.\n",
    "- **Highly biased datasets** amplify existing imbalances.\n",
    "- In contrast, **larger and more balanced datasets** reduce amplification risk.\n",
    "\n",
    "#### Confidence and Calibration\n",
    "\n",
    "**Poor calibration** means the model’s prediction confidence does **not match** its actual accuracy.\n",
    "\n",
    "- Overconfident models — especially on **underrepresented groups** — are more likely to amplify bias.\n",
    "- A model that makes incorrect predictions with **high confidence** can mask its errors and further reinforce group disparities.\n",
    "- This effect is particularly common in **high-capacity models**.\n",
    "\n",
    "**Important:**  \n",
    "The findings above are based on **binary classification** and **image recognition tasks**.  \n",
    "> **More research is needed** to determine how generalizable these patterns are across domains and applications.\n",
    "\n",
    "The next section puts the theoretical bias types from this page into practice. We analyze gender classification models to investigate the different forms of bias in a real-world scenario.\n",
    "\n",
    "---\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**1. True or False:**\n",
    "Deployment bias occurs when the training data is not representative of the target population.\n",
    "1. [ ] True\n",
    "2. [ ] False\n",
    "\n",
    "**2. Which of the following is an example of measurement bias?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] A model trained on mostly Western images underperforms in Asian countries\n",
    "2. [ ] A recidivism model uses \"arrest record\" as a label for \"criminal behavior\"\n",
    "3. [ ] A model misclassifies women due to male-dominated training data\n",
    "4. [ ] A model is evaluated only on benchmark datasets that exclude minorities\n",
    "\n",
    "**3. Which statement about bias amplification is correct?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] It only occurs when the training data is fully biased\n",
    "2. [ ] It can occur even in small datasets due to overfitting\n",
    "3. [ ] It always increases with model capacity\n",
    "4. [ ] It can be avoided by removing all group-related features\n",
    "\n",
    "---\n",
    "\n",
    "#### Sources:\n",
    "1. Mehrabi et al., 2021\n",
    "2. Howard & Borenstein, 2018\n",
    "3. Suresh & Guttag, 2021\n",
    "4. Corbett-Davies et al., 2023\n",
    "5. Buolamwini & Gebru, 2018\n",
    "6. Hall et al., 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
