{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720b90b6-56ac-4e12-9837-871ad02e02f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ethical Challenges in Machine Learning\n",
    "## A practical guide addressing issues such as bias, fairness, and self-fulfilling predictions\n",
    "\n",
    "This notebook has two main goals:\n",
    "\n",
    "1. **Raising awareness** of the ethical risks in machine learning (ML) and why fairness must be considered when designing or using ML systems.\n",
    "2. Demonstrate why **achieving fairness** is complex, often involving trade-offs and difficult decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Structure\n",
    "1. Motivation\n",
    "2. Measuring Fairness\n",
    "3. Practical Application of Fairness Metrics\n",
    "4. Bias in Machine Learning\n",
    "5. Identifying Bias in Gender Classification\n",
    "6. Self-Fulfilling Predictions & Feedback Loops\n",
    "7. Challenges and Recommendations for Fairness in Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Motivation\n",
    "\n",
    "Machine learning applications are becoming more important in modern decision-making.\n",
    "They influence everyday tasks like search rankings or personalized recommendations, but also high-stake scenarios with big impact on individuals, such as hiring, healthcare, education or law enforcement.\n",
    "Errors in such domains need to be recognized and mitigated. \n",
    ">If machine learning models are not carefully designed and monitored, they can **reinforce existing biases**, **contribute to discrimination**, and produce **unfair outcomes** that affect some societal groups more than others.<sup>1</sup>\n",
    "\n",
    "What makes this even more concerning is that it often happens **unintentionally**.\n",
    "\n",
    "Before we get deeper into what fairness means and how it can be accessed, we begin with one of the most widely discussed real-world examples of unfairness in algorithmic systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Case Study: COMPAS - Risk Prediction in Criminal Justice\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Imagine you are working for a court.\n",
    "You are introduced to a tool called **COMPAS** (Correctional Offender Management Profiling for Alternative Sanctions). It predicts the likelihood that a defendant will commit another crime. \n",
    "\n",
    "The goal: **help judges make better & fairer decisions.**\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: First Impression\n",
    "\n",
    "COMPAS uses a questionnaire with **130+ factors** to predict a risk score between 1 (low) and 10 (high). \n",
    "\n",
    "Here is what the developer claims:<sup>2</sup>\n",
    "\n",
    "- It **does not use race** as an input.\n",
    "- It has been **statistically validated for overall predictive accuracy**, meaning that COMPAS risk scores correlate with actual reoffending rates across the entire population.\n",
    "- It aims to **help reduce human biases** in the justice system.\n",
    "\n",
    "\n",
    "> **What is a Sensitive Attribute?**<sup>1</sup>  \n",
    "> \n",
    "> A sensitive attribute relates to protected or vulnerable characteristics of individuals, such as race, gender, age, religion, disability status, or sexual orientation.  \n",
    "> \n",
    "> Discrimination based directly on sensitive attributes is considered ethically unacceptable and often prohibited by law.\n",
    "\n",
    "\n",
    "#### Reflection:\n",
    "*Based on this description, would you trust COMPAS as a fair and objective tool?*\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Exploring the Risk Scores\n",
    "\n",
    "Below, you see histograms of risk scores from two groups:<sup>2</sup>\n",
    "\n",
    "- First Black defendants\n",
    "- Second White defendants\n",
    "\n",
    "![](Images/Black.png) ![](Images/White.png)\n",
    "\n",
    "#### Reflection:\n",
    "*Do you notice any differences between the groups?*\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Investigating Individual Cases\n",
    "\n",
    "You now review some real examples where COMPAS was used:<sup>2</sup>\n",
    "\n",
    "|              | ![](Images/Vernon.png)   | ![](Images/Brisha.png) | \n",
    "|----------------|--------------------|--------------------|\n",
    "| **Prior Offenses** | 2 armed robberies,<br> 1 attempted armed robbery | 4 juvenile misdemeanors     |\n",
    "| **Subsequent Offenses** | 1 grand theft    | None      |\n",
    "\n",
    "\n",
    "|                | ![](Images/Dylan.png) | ![](Images/Bernhard.png) |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Prior Offenses** | 1 attempted burglary | 1 resisting arrest without violence     |\n",
    "| **Subsequent Offenses** | 3 drug possessions    | None      |\n",
    "\n",
    "|                | ![](Images/James.png) | ![](Images/Robert.png) |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Prior Offenses** | 1 domestic violence assault,<br> 1 grand theft, 1 petty theft,<br> 1 drug trafficking | 1 petty theft     |\n",
    "| **Subsequent Offenses** | 1 grand theft    | None      |\n",
    "\n",
    "#### Reflection:\n",
    "*Do the assigned risk scores match your expectations based on prior and subsequent offenses?*\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Findings from ProPublica Investigation\n",
    "\n",
    "In 2016 **Angwin et al.** analyzed the fairness of COMPAS and found significant **racial bias** in its predictions. The most important aspects are summarized below ([full article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)).<sup>2</sup>\n",
    "\n",
    "#### Key Findings:\n",
    "\n",
    "|                | White Defendants | Black Defendants |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Predicted High Risk, No Reoffense (False Positive)** | 23.5%      | 44.9%     |\n",
    "| **Predicted Low Risk, Reoffended (False Negative)** | 47.7%     | 28.0%      |\n",
    "\n",
    "- Black defendants were **almost twice as likely** to be incorrectly labeled high-risk compared to white defendants.\n",
    "- White defendants were **more often** incorrectly labeled low-risk.\n",
    "- These disparities remained **even after accounting for** prior offenses, age, and gender.\n",
    "- Although race was **not an explicit input**, bias emerged indirectly through correlated variables such as education, employment, or neighborhood (proxies).\n",
    "\n",
    "---\n",
    "\n",
    "### Broader Ethical Issues Identified\n",
    "\n",
    "- **Seemingly neutral algorithms can reinforce societal inequalities**\n",
    ">Even if a model does not use race directly, it can lead to structural disadvantages through proxy variables like income, neighborhood, or education level. This makes it possible for biases to persist invisibly within algorithmic decision-making.\n",
    "\n",
    "- **Intended fairness is not sufficient**  \n",
    ">Although the system excluded race to reduce bias, it still produced racially biased results. This shows that simply omitting sensitive features does not automatically prevent discrimination when proxy variables exist.\n",
    "\n",
    "- **Historical data can encode and amplify structural inequalities**  \n",
    ">Machine learning models often learn patterns from past decisions, which may reflect biased practices.  \n",
    "Without critical oversight, such models can replicate or even reinforce these biases.\n",
    "\n",
    "- **Lack of transparency and explainability**  \n",
    ">As a proprietary system, COMPAS offers no insight into how its risk scores are generated.  \n",
    "This opacity makes it difficult for affected individuals to understand or contest decisions — reducing trust in the system.\n",
    "\n",
    "- **No clear accountability**  \n",
    ">When algorithmic decisions lead to harmful outcomes, responsibility is often unclear.  \n",
    "Is it the developers, the institutions that deploy the system, or the data providers who should be held accountable? Accountability is often necessary to implement corrective actions.\n",
    "\n",
    "- **Subtle and invisible bias**  \n",
    ">Algorithmic bias often operates below the surface through indirect correlations and statistical patterns.  \n",
    "Affected individuals often don't realize that they have been treated unfairly and such systems can remain unchallenged for long periods.\n",
    "\n",
    "---\n",
    "\n",
    "### Conflict Between Fairness Definitions\n",
    "\n",
    "The COMPAS case also illustrates that **different definitions of fairness can be in conflict**:<sup>3</sup>\n",
    "\n",
    "- The company behind COMPAS argued that the tool was **calibrated**:\n",
    "  Among individuals with the same risk score, the probability of reoffending was similar across racial groups.\n",
    "\n",
    "- Angwin et al. (2016) emphasized **unequal error rates**:\n",
    "  Black defendants had much higher false positive rates, and white defendants had higher false negative rates — a violation of **equalized odds**.\n",
    "\n",
    "> An introduction into fairness metrics such as calibration and equalized odds will follow in the next part of this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "The COMPAS case shows that even when algorithms are intended to be neutral and fair, they can still replicate and even reinforce societal inequalities.\n",
    "\n",
    "The identified issues highlight ethical challenges that can arise when deploying machine learning systems in high-stakes areas.\n",
    "\n",
    "> This demonstrates why an **ethical perspective is not optional, but essential** when developing and using machine learning, especially when these systems have a direct impact on people's lives. \n",
    "\n",
    "Machine learning makes decisions based on **statistical inference**. Algorithmic decisions use **generalizations** and fail to treat people as individuals by design. While such generalizations can be statistically sound and necessary, they can only be morally acceptable if they are sufficiently **accurate** and do **not create systematic disadvantages**.<sup>4</sup>\n",
    "\n",
    "In the next part of this notebook, we will take a closer look at what fairness in machine learning actually means and why defining fairness is itself a complex task.\n",
    "\n",
    "---\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**1. True or False:**\n",
    "Excluding sensitive attributes like race guarantees that a machine learning model will be fair.\n",
    "1. [ ] True\n",
    "2. [ ] False\n",
    "\n",
    "**2. Which of the following best describes why the COMPAS tool was criticized by Angwin et al.?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] It was completely inaccurate in predicting any reoffending\n",
    "2. [ ] It explicitly used race as an input feature\n",
    "3. [ ] It showed different error rates between racial groups\n",
    "4. [ ] It was free and open-source, causing legal concerns\n",
    "\n",
    "**3. Which of the following actions would most likely help prevent biased outcomes like those found in the COMPAS case?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] Removing sensitive attributes from the model\n",
    "2. [ ] Using more training data, regardless of their correlations\n",
    "3. [ ] Carefully auditing how features correlate with sensitive attributes\n",
    "4. [ ] Optimizing the model only for highest predictive accuracy across the entire population\n",
    "   \n",
    "---\n",
    "\n",
    "#### Sources:\n",
    "1. Mehrabi et al., 2021\n",
    "2. Angwin et al., 2016\n",
    "3. Barocas et al., 2023\n",
    "4. Binns, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f23e3-e95e-4d64-af3b-fe3bb2d7bdf9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
