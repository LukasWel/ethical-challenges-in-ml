{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683ddeee-f2e9-41f7-8cd9-6d68fa1a0ff6",
   "metadata": {},
   "source": [
    "# Part 2: Measuring Fairness\n",
    "\n",
    "## Fairness and Discrimination\n",
    "Fairness and discrimination are closely related, but not the same. \n",
    "\n",
    ">**Fairness** describes the goal of treating individuals and groups equitably in decision-making processes. \n",
    ">**Discrimination** refers to a violation of this goal, often resulting in unjust or unequal outcomes<sup>1</sup>. \n",
    "\n",
    "In the context of machine learning, fairness is used to assess whether algorithmic decisions disadvantage certain groups — especially based on sensitive attributes like race, gender, or age.\n",
    "Several factors influence fairness, such as:\n",
    "\n",
    "- **Transparency**\n",
    "- **Accountability**\n",
    "- **Explainability**\n",
    "- **Bias**\n",
    "\n",
    "> **Among these, bias plays the most significant role in contributing to discrimination.**\n",
    "\n",
    "\n",
    "There is **no universal definition of fairness** in machine learning (neither in other disciplines). Competing perspectives exist, often shaped by legal, cultural, or domain-specific goals.<sup>2</sup>\n",
    "\n",
    "- **Individual fairness**: Similar individuals should be treated similarly.\n",
    "- **Group fairness**: Different demographic groups should receive similar outcomes.<sup>3</sup>\n",
    "\n",
    "Each approach has **trade-offs**. \n",
    "What is fair in credit scoring may not be fair in university admissions or criminal justice.<sup>4</sup> \n",
    "\n",
    "Discrimination in machine learning is different from traditional human discrimination. Algorithms do not have intent or moral awareness. However, they can still produce discriminatory outcomes when trained on biased or unbalanced data. We can distinguish two main types of algorithmic discrimination:<sup>3</sup>\n",
    "\n",
    "- **Direct discrimination**: Using protected attributes (e.g. gender or race) explicitly in decision-making.\n",
    "- **Indirect discrimination**: Using seemingly neutral features (e.g. zip code) that correlate with protected attributes and act as proxies.<sup>2</sup>\n",
    "\n",
    "Discrimination occurs at multiple levels:<sup>1</sup>\n",
    "\n",
    "- **Structural**: Systemic inequality embedded in laws or history\n",
    "- **Organizational**: Biased rules or decision processes in institutions\n",
    "- **Interpersonal**: Individual-level stereotypes or assumptions\n",
    "\n",
    "Machine learning systems can unintentionally replicate or amplify discrimination from any of these levels.\n",
    "Understanding how fairness and discrimination interact is essential for designing ethical models. The next section introduces common fairness metrics that allow us to detect and evaluate discrimination in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## Fairness Metrics\n",
    "\n",
    "As already said, there is no universal definition of fairness in machine learning. Over the past years, many different fairness metrics have been proposed to evaluate algorithmic decision-making. The large number of fairness definitions can be overwhelming, also because there is no clear consensus on when to use which metric. Simply satisfying as many notions as possible is not an option, as some definitions  are **mathematically incompatible**.<sup>5,</sup><sup>6</sup> \n",
    "> **Fairness metrics should be seen as diagnostic tools, not automatic solutions. They can help to identify potential sources of unfairness and discrimination but do not directly fix them.** \n",
    "\n",
    "Choosing the right metric depends on:\n",
    "- **The application domain**\n",
    "- **Ethical priorities**\n",
    "- **Context-specific trade-offs**\n",
    "\n",
    "The following section focuses on **observational fairness metrics**. As many of these definitions are derived from the confusion matrix, first a small reminder about the **confusion matrix** and some **core statistical measures**.\n",
    "\n",
    "The selected fairness metrics presented in this part of the notebook represent the main ideas behind measures of fairness, as many of them are similar in their approach. For a more extensive list of fairness metrics see the paper by Verma & Rubin (2018). Also note that most research on fairness metrics (and also this notebook) focuses on classification algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix compares the predicted and true class labels. It forms the basis for many fairness and performance metrics:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)      | False Negative (FN)     |\n",
    "| **Actual Negative** | False Positive (FP)     | True Negative (TN)      |\n",
    "\n",
    "---\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is the most basic evaluation metric in classification:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "> **Limitation**: Accuracy can be misleading in imbalanced datasets and tells us nothing about fairness across groups.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Statistical Measures\n",
    "\n",
    "These 8 measures are derived from the confusion matrix and form the basis for many fairness definitions:<sup>7</sup>\n",
    "\n",
    "| **Measure**                          | **Formula**               | **Description**                                      |\n",
    "|-------------------------------------|---------------------------|------------------------------------------------------|\n",
    "| Positive Predictive Value/Precision (PPV)  | TP / (TP + FP)     | How many predicted positives are correct? |\n",
    "| False Discovery Rate (FDR)          | FP / (TP + FP)            | How many predicted positives are wrong?   |\n",
    "| Negative Predictive Value (NPV)     | TN / (TN + FN)            | How many predicted negatives are correct? |\n",
    "| False Omission Rate (FOR)           | FN / (TN + FN)            | How many predicted negatives are wrong?   |\n",
    "| True Positive Rate/Recall/Sensitivity (TPR)| TP / (TP + FN)     | How many actual positives are caught?     |\n",
    "| False Negative Rate (FNR)           | FN / (TP + FN)            | How many positives were missed?           |\n",
    "| False Positive Rate (FPR)           | FP / (FP + TN)            | How many negatives were wrongly predicted as positive?|\n",
    "| True Negative Rate/Specificity (TNR)| TN / (TN + FP)            | How many actual negatives are caught?     |\n",
    "\n",
    "> These values are computed **per group** (e.g. male/female) to assess fairness.\n",
    "\n",
    "---\n",
    "\n",
    "## Observational Fairness\n",
    "\n",
    "**Observational fairness** refers to fairness definitions that rely only on observed data — specifically on statistical relationships between:\n",
    "\n",
    "- **Ŷ**: the model prediction  \n",
    "- **Y**: the ground truth  \n",
    "- **A**: the sensitive attribute (e.g. race, gender)\n",
    "\n",
    "These definitions **do not require access to causal knowledge** or model internals. They are easy to compute and widely used in fairness audits.\n",
    "\n",
    "Most observational fairness metrics are based on combinations of the 8 statistical measures from the confusion matrix.\n",
    "\n",
    "We group them into **three main categories**:<sup>1</sup>\n",
    "\n",
    "- Independence\n",
    "- Separation\n",
    "- Sufficiency\n",
    "\n",
    "---\n",
    "\n",
    "### Independence\n",
    "\n",
    "Requires:  \n",
    "$$\n",
    "\\hat{Y} \\perp A\n",
    "$$\n",
    "\n",
    "The predicted outcome should be statistically independent of the sensitive attribute.<sup>1</sup>\n",
    "\n",
    "This means that all groups should receive positive predictions at equal rates — **regardless of their actual outcome (Y)**.<sup>7</sup>\n",
    "\n",
    "#### Common Metric\n",
    "- **Statistical Parity / Demographic Parity**<sup>7</sup>\n",
    "  $$\n",
    "  P(\\hat{Y} = 1 \\mid A = 0) = P(\\hat{Y} = 1 \\mid A = 1)\n",
    "  $$\n",
    "\n",
    "#### Pros\n",
    "- Simple and intuitive\n",
    "- Easy to implement and visualize\n",
    "\n",
    "#### Cons\n",
    "- Ignores true outcome (Y)\n",
    "- Can result in unfair treatment if base rates differ\n",
    "- Overlooks explainable or justified outcome differences\n",
    "- Blind to structural and historical context\n",
    "\n",
    "> **Example**:\n",
    "> Consider a machine learning system that predicts the likelihood of reoffending (such as COMPAS).\n",
    "> If men statistically reoffend more often than women, enforcing equal prediction rates across groups could lead to harsher treatment of women without justification.<sup>4</sup>\n",
    "\n",
    "---\n",
    "\n",
    "### Separation\n",
    "\n",
    "Requires:  \n",
    "$$\n",
    "\\hat{Y} \\perp A \\mid Y\n",
    "$$\n",
    "\n",
    "Given the true label Y, predictions should be independent of A.\n",
    "\n",
    "This ensures **equal error rates** across groups.<sup>1</sup>\n",
    "\n",
    "#### Common Metrics\n",
    "- **Equalized Odds**:<sup>7</sup>  \n",
    "  Equal FPR and FNR across groups  \n",
    "- **Predictive Equality**:<sup>7</sup>\n",
    "  Equal FPR only\n",
    "- **Equal Opportunity**:<sup>7</sup>\n",
    "  Equal FNR only\n",
    "\n",
    "#### Pros\n",
    "- Captures error disparities between groups\n",
    "- Especially relevant when different types of errors (false positives vs. false negatives) have unequal real-world consequences (e.g. false arrests vs. wrongful releases)\n",
    "\n",
    "#### Cons\n",
    "- May reduce overall accuracy\n",
    "- Relies on a valid and unbiased ground truth (Y)\n",
    "- Cannot be satisfied together with other metrics under realistic conditions\n",
    "\n",
    "> Trade-offs between fairness and performance can be visualized using **ROC curves**.\n",
    "> Disparities in error rates often reflect and reinforce historical inequalities.<sup>1</sup>\n",
    "\n",
    "---\n",
    "\n",
    "### Sufficiency\n",
    "\n",
    "Requires:  \n",
    "$$\n",
    "Y \\perp A \\mid \\hat{Y}\n",
    "$$\n",
    "\n",
    "Given the prediction, the true outcome should be independent of A.\n",
    "\n",
    "This means that **predictions are equally reliable across groups**.<sup>1</sup>\n",
    "\n",
    "#### Common Metrics\n",
    "- **Calibration Concepts:<sup>7</sup>**\n",
    "\n",
    "   - **Calibration**:\n",
    "     On average, across the whole population, predicted probabilities match actual outcomes.\n",
    "\n",
    "    - **Group Calibration**:\n",
    "     Predicted probabilities match actual outcomes **within each demographic group** (e.g. gender, race).\n",
    " \n",
    "    - **Well-Calibration**:\n",
    "      The strongest form — predicted probabilities perfectly match actual outcomes at a fine-grained level **within each group** and for **each score value**.\n",
    "  \n",
    "  In fairness assessments, **group calibration** is mostly used because it checks whether predictions are **equally interpretable across groups**.\n",
    " \n",
    "> **Example:**\n",
    "> If a model predicts a 70% risk of default, about 70% of individuals assigned a score of 0.7 should actually default — regardless of their group membership.  \n",
    "  \n",
    "- **Predictive Parity**:<sup>7</sup>  \n",
    "  Equal PPV across groups  \n",
    " \n",
    "#### Pros\n",
    "- Predictions are equally reliable across groups\n",
    "- Ensures consistent interpretation of predicted scores\n",
    "- Often achievable without explicit fairness constraints\n",
    "\n",
    "#### Cons\n",
    "- Relies on a valid and unbiased ground truth (Y)\n",
    "- Can reproduce harmful disparities if the ground truth itself reflects historical bias\n",
    "- Can conflict with separation-based metrics\n",
    "\n",
    "> Sufficiency ensures that prediction scores are consistent across groups, but it does not address deeper structural inequalities (but this is true for all statistical fairness definitions).<sup>1</sup>\n",
    "\n",
    "---\n",
    "\n",
    "### Incompatibility of Metrics\n",
    "\n",
    "One challenge in fair machine learning is that **not all fairness metrics can be satisfied at the same time**. In many real-world situations, especially because the sensitive attribute A and the true outcome Y are almost always statistically dependent, fairness definitions make **conflicting assumptions**.\n",
    "Conflicts arise because:\n",
    "\n",
    "- **Independence** requires predictions to ignore group membership\n",
    "$$\n",
    "\\hat{Y} \\perp A\n",
    "$$\n",
    "- **Separation** requires equal error rates, which depend on group-specific outcome distributions\n",
    "$$\n",
    "\\hat{Y} \\perp A \\mid Y\n",
    "$$\n",
    "- **Sufficiency** focuses on reliability of predictions across groups\n",
    "$$\n",
    "Y \\perp A \\mid \\hat{Y}\n",
    "$$\n",
    "\n",
    "\n",
    "These conditions can't hold simultaneously unless:<sup>5,</sup><sup>6</sup>\n",
    "\n",
    "- Predictions are **perfect**, or\n",
    "- The sensitive attribute **has no statistical relationship** with the target variable (which is rare)\n",
    "\n",
    "Example: **COMPAS**<sup>8</sup>\n",
    "\n",
    "- The model was **calibrated** → it satisfied sufficiency (PPV equal across groups)\n",
    "- But it had **unequal error rates** (FPR/FNR differed by race) → it violated separation\n",
    "- This illustrates how calibration alone cannot guarantee fairness.<sup>1</sup>\n",
    "\n",
    "**Takeaway:** Fairness is not one-size-fits-all. Trade-offs between fairness goals are inevitable. Which metric to use depends on context, goals, and ethical priorities.\n",
    "\n",
    "> **Note:** Observational fairness metrics are useful diagnostic tools to detect statistical disparities between groups.  \n",
    "> However, they only rely on observed relationships between predictions, outcomes, and sensitive attributes — and ignore other relevant features that may contribute to unfairness.  \n",
    "> As a result, they often miss the actual mechanisms behind discrimination. These metrics are blind to structural inequalities, biased decision processes, and causal factors outside the model.  \n",
    "> They can confirm unequal treatment, but not explain *why* it happens.  \n",
    "> **Similarity-based** and **causal fairness** approaches aim to address these limitations by evaluating the decision-making process itself and identifying justified and unjustified sources of disparity.\n",
    "\n",
    "---\n",
    "\n",
    "## Similarity-Based Fairness\n",
    "\n",
    "While observational fairness focuses on statistical group-level patterns in model outcomes, **similarity-based fairness** evaluates the fairness of the **decision process itself** by assessing whether similar individuals receive similar outcomes — regardless of their group membership.\n",
    "\n",
    "> This reflects the intuitive idea that fairness means treating comparable cases consistently, based on individual characteristics.<sup>2</sup>\n",
    "\n",
    "**Note:** Defining what counts as *similar* can itself be subjective and context-dependent.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "#### Causal Discrimination (pairwise test)<sup>7</sup>\n",
    "- Two individuals who differ **only** in a sensitive feature (e.g. gender) should receive **the same outcome**.\n",
    "- Captures the idea that sensitive attributes **should not causally influence** decisions.\n",
    "- Often unrealistic in practice to find two individuals that differ only in one dimension, as sensitive attributes are usually correlated with other features.\n",
    "\n",
    "#### Fairness through Unawareness<sup>9</sup>\n",
    "- Ensures fairness by **removing sensitive attributes** from the data (blinding).\n",
    "- Based on the logic: “if the model doesn’t see it, it can’t discriminate.”\n",
    "- Limitation: **proxy variables** can still leak bias (e.g. in the U.S. zip code → race).\n",
    "- Can lead to **miscalibration** or **reduced accuracy** for some groups if relevant factors are ignored.\n",
    "\n",
    "#### Fairness through Awareness<sup>7</sup>\n",
    "- Uses a **similarity function** (e.g. distance metric) to compare individuals.\n",
    "- Individuals who are “close” in feature space should be treated similarly.\n",
    "- Requires including **all relevant attributes**, including sensitive ones.\n",
    "- More flexible than blinding, but depends on how similarity is defined.\n",
    "\n",
    "---\n",
    "\n",
    "#### Pros\n",
    "- Evaluates fairness at the **individual level**, not just between groups\n",
    "- Focuses on the **decision process**, not only on outcomes\n",
    "- Allows for **context-sensitive** definitions of fairness\n",
    "- Bridges the gap between observational and causal fairness approaches\n",
    "\n",
    "#### Cons\n",
    "- Defining **similarity functions** is subjective and challenging\n",
    "- Sensitive attributes often correlate with other features, complicating comparisons\n",
    "- Requires **complex feature engineering** and **domain knowledge**\n",
    "\n",
    "> Similarity-based fairness emphasizes how decisions are made rather than just focusing on the final outcomes. It requires careful thinking about which individuals should be treated similarly.<sup>7</sup>\n",
    "\n",
    "---\n",
    "\n",
    "## Causal Fairness\n",
    "\n",
    "Causal fairness uses **causal models** to understand how variables (e.g. gender, income, education) influence decisions and to **distinguish fair from unfair causal effects**.\n",
    "\n",
    "> Instead of just asking *who gets what*, causal models ask *why* certain outcomes occur — and whether sensitive attributes **legitimately** influence decisions.\n",
    "\n",
    "Causal models are represented as **directed graphs** and allow for:<sup>2</sup>\n",
    "- Reasoning about **hypothetical scenarios**\n",
    "- Identifying and blocking **unfair influence paths**\n",
    "- Designing **interventions** to improve fairness\n",
    "\n",
    "However, modeling causality for **social attributes** (e.g. race, gender) is challenging:<sup>1</sup>\n",
    "- These categories are **socially constructed**, not fixed\n",
    "- Meanings vary across time, cultures, and contexts\n",
    "- Looping effects exist: being labeled can influence future behavior\n",
    "\n",
    "---\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "#### Counterfactual Fairness<sup>9</sup>\n",
    "- Asks: *Would the decision have been different if the person had belonged to another group when everything else is equal?*\n",
    "- If the answer is *no* the decision is fair\n",
    "- Enables **individual-level fairness auditing**\n",
    "- Requires detailed causal models and well-defined counterfactuals\n",
    "\n",
    "#### Path-Specific Fairness<sup>10</sup>\n",
    "- Recognizes that **some paths** from sensitive attributes to outcomes may be acceptable and others not\n",
    "- Allows defining which **causal paths are fair**\n",
    "- Offers a **flexible and context-aware** balance between utility and fairness\n",
    "\n",
    "---\n",
    "\n",
    "#### Pros\n",
    "- Enables **targeted diagnosis** of unfair influence  \n",
    "- Works even with indirect or subtle bias  \n",
    "- Supports **intervention design**\n",
    "\n",
    "#### Cons\n",
    "- Requires **domain knowledge** and modeling effort  \n",
    "- Difficult to apply when variables are **entangled** or **not clearly defined**\n",
    "\n",
    "\n",
    "> Causal fairness provides the **most powerful and flexible tools** for fairness analysis, but also the most demanding in terms of assumptions and modeling effort.\n",
    "\n",
    "The next section provides a practical example showing how different fairness metrics can stand in **conflict** with each other and why **no single metric** is sufficient for evaluating fairness in complex systems. In practice, a combination of observational, similarity-based, and causal fairness assessments can offer a more comprehensive understanding of bias and discrimination in ML systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**1. True or False:**\n",
    "If a model satisfies statistical parity, it also guarantees equal error rates across groups.\n",
    "1. [ ] True\n",
    "2. [ ] False\n",
    "\n",
    "**2. Which of the fairness metrics requires predictions to be independent of the sensitive attribute, regardless of the true outcome?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] Equalized Odds\n",
    "2. [ ] Statistical Parity\n",
    "3. [ ] Calibration\n",
    "4. [ ] Predictive Parity\n",
    "\n",
    "**3. Which statement about the incompatibility of fairness metrics is correct?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] All fairness metrics can usually be satisfied simultaneously in real-world settings\n",
    "2. [ ] If predictions are imperfect and sensitive attributes influence the outcome, different fairness goals can be in conflict\n",
    "3. [ ] Independence, Separation, and Sufficiency are not in conflict when sensitive attributes are statistically related to the outcome\n",
    "4. [ ] Sufficiency and Separation can always be satisfied together if enough data is available\n",
    "\n",
    "---\n",
    "\n",
    "#### Sources:\n",
    "1. Barocas et al., 2023\n",
    "2. Mehrabi et al., 2021\n",
    "3. Calegari et al., 2023\n",
    "4. Binns, 2018\n",
    "5. Chouldechova, 2017\n",
    "6. Kleinberg et al., 2016\n",
    "7. Verma & Rubin, 2018\n",
    "8. Angwin et al., 2016\n",
    "9. Kusner et al., 2017\n",
    "10. Corbett-Davies et al., 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5951a-6eab-46ae-9112-d0c550bde326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
