{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d096d2ff-d6ee-4bf9-ade4-122e729d593f",
   "metadata": {},
   "source": [
    "# Part 7: Challenges and Recommendations for Fairness in Machine Learning\n",
    "\n",
    "## Why Improving Fairness is Hard\n",
    "\n",
    "Fairness in machine learning is not a one-time technical fix.  \n",
    "It is a **continuous, context-sensitive process** that must balance competing values and account for the real-world effects of algorithmic decisions.\n",
    "\n",
    "This section highlights **five major challenges** in achieving fairness — each one illustrated through the three case studies:  \n",
    "**student dropout prediction**, **gender classification**, and **predictive policing**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Conceptual limitations of fairness definitions\n",
    "\n",
    "Fairness in machine learning is often defined using mathematical metrics like **Equalized Odds** or **Calibration**.  \n",
    "These definitions are useful diagnostic tools but they come with **mathematical and conceptual limitations**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Correlation vs. Causality<sup>16</sup>\n",
    "\n",
    "Observational fairness metrics are based on **statistical correlations**, not **causal relationships**. \n",
    "Machine learning models trained on observational data often learn patterns that **reflect existing inequalities**, rather than their causes.\n",
    "\n",
    "**Example: Predictive Policing**\n",
    "\n",
    "A model might learn that certain neighborhoods have higher crime rates — not because they are more dangerous, but because they are more heavily policed.\n",
    "Treating such correlations as causal can reinforce biased enforcement strategies and fail to address the actual causes of crime.\n",
    "\n",
    ">Fairness interventions that ignore causal mechanisms risk producing **harmful or misleading outcomes**.\n",
    "Whenever possible, **causal reasoning** should complement statistical fairness metrics — even though it requires strong **domain knowledge** and may not always be feasible.\n",
    "\n",
    "---\n",
    "\n",
    "#### Incompatibility of metrics\n",
    "\n",
    "As already discussed in previous sections, some fairness metrics are **mathematically incompatible**. It is often impossible to satisfy both simultaneously unless the model is perfect or group base rates are identical.<sup>1,</sup><sup>2</sup>\n",
    "\n",
    "**Example: Student Dropout Prediction**:\n",
    "\n",
    "Enforcing **Separation** via threshold adjustment led to a loss in **Sufficiency**.  \n",
    "This meant that predicted risk scores no longer had a consistent meaning across groups.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Inframarginality<sup>3</sup>\n",
    "\n",
    "Many fairness metrics, such as Statistical Parity or Predictive Equality, operate on **infra-marginal statistics**. They summarize model performance **across the whole distribution**, rather than focusing on **decisions near the threshold**, where real-world consequences are greatest.\n",
    "\n",
    "**Example: Diabetes Screening (Corbett-Davies et al., 2023)**  \n",
    "Suppose a single threshold (e.g., 1.5% risk) is used to screen for diabetes.  \n",
    "Groups with **higher baseline risk** (e.g. Asian Americans) will be screened more often — maximizing **overall health outcomes**.  \n",
    "But this **violates Statistical Parity**, because not all groups are screened at the same rate.\n",
    "\n",
    "Now, if we **adjust the thresholds** to **enforce parity**, we must:\n",
    "- Raise the threshold for high-risk groups → they lose access to needed care.\n",
    "- Lower the threshold for low-risk groups → they receive unnecessary tests.\n",
    "\n",
    "**The result: every group is worse off.**\n",
    "\n",
    "This fairness intervention leads to a situation where a different policy (e.g. keeping a uniform threshold) would produce better outcomes **for all**. \n",
    "\n",
    "---\n",
    "\n",
    "#### Pareto-dominated outcomes<sup>3</sup>\n",
    "\n",
    "As Corbett-Davies et al. (2023) argue, fairness interventions that **optimize infra-marginal metrics** (like Statistical Parity) can lead to **pareto-dominated policies**.\n",
    "\n",
    "A policy is pareto-dominated when an **alternative exists that would improve outcomes for all groups** — but is rejected due to the fairness constraint.\n",
    "\n",
    "**See example above: Diabetes Screening**  \n",
    "Enforcing statistical parity led to **worse outcomes for every group**, even though the metric was satisfied.  \n",
    "The fairness intervention was **pareto-inefficient**: all groups could have been better off under the original threshold.\n",
    "\n",
    "**General insight:**  \n",
    "This shows that **fairness metrics detached from real-world utility** can be misleading.  \n",
    "> **Satisfying a metric ≠ Fair outcome**\n",
    "\n",
    "Fairness should not be judged solely by mathematical parity but by **whether interventions improve outcomes for the intended groups** — especially at the decision margin.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight:**  \n",
    "- Many standard fairness metrics rely on simplified assumptions and ignore what happens at the decision boundary.  \n",
    "- This can lead to **pareto-dominated outcomes**, where **no group benefits**, even if fairness metrics are satisfied.  \n",
    "- Therefore, **fairness interventions should not be judged solely by mathematical definitions**, but by whether they **actually improve outcomes for the people involved**.\n",
    "\n",
    "This calls for a **consequentialist perspective** on fairness:<sup>3</sup>  \n",
    "Instead of optimizing metrics in isolation, we must ask:  \n",
    "> *“What are the real-world effects of this intervention – and who benefits or is harmed?”*\n",
    "\n",
    "Only by evaluating fairness in terms of **context, utility, and impact** can machine learning systems be designed to support equitable outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Context dependence of fairness metrics\n",
    "\n",
    "Fairness definitions are **not universal**, they depend on **application domain**, **harms**, and **social context**.\n",
    "What is a fair decision in one setting may be inappropriate in another.<sup>4,</sup><sup>5</sup>\n",
    "\n",
    "**Example: Student Dropout Prediction**\n",
    "\n",
    "Choosing between fairness metrics (e.g. Equal Opportunity vs. Predictive Parity) required ethical reflection about the **consequences** for students.\n",
    "\n",
    "**Example: Gender Classification**  \n",
    "\n",
    "Standard metrics like Statistical Parity fail here.  \n",
    "**Representational harms** need other forms of assessment, like intersectional accuracy, and take visibility into account.\n",
    "\n",
    "**Example: Predictive Policing**  \n",
    "\n",
    "Even if a model minimizes overall error, it may reinforce **historical over-policing**.  \n",
    "Fairness here must account for political and social history. Evaluating clustering requires a different procedure and cannot be captured by standard classification fairness metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Representation and measurement issues\n",
    "\n",
    "Bias often stems from **how the world is represented** in data and labels — not just how the model learns.<sup>6</sup>\n",
    "\n",
    "**Example: Student Dropout Prediction**  \n",
    "\n",
    "The label “dropout” may reflect financial or structural **disadvantages** — not actual academic ability.  \n",
    "Models risk learning that \"disadvantage = failure\".\n",
    "\n",
    "**Example: Gender Classification**  \n",
    "\n",
    "Gender is not always binary or observable. Labels in datasets like FairFace are often **annotator-assigned**, not **self-identified**.  \n",
    "This raises issues of **label validity** and representational fairness.<sup>27</sup>\n",
    "\n",
    "**Example: Predictive Policing**  \n",
    "\n",
    "Training on historical police data reflects where police were active, not necessarily where crime occurred.  \n",
    "Models predict **policing behavior**, not **crime risk**, reinforcing **feedback loops**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Lack of standards, transparency, and accountability\n",
    "\n",
    "There are **no consistent standards** for evaluating fairness.<sup>7,</sup><sup>8</sup>  \n",
    "Without transparency or clear responsibilities, fairness becomes difficult to assess or enforce.\n",
    "\n",
    "**Example: Student Dropout Prediction**  \n",
    "\n",
    "There is **no agreement** on which metric to use.  \n",
    "Without standards, fairness choices become arbitrary and students may not understand or contest the decision.\n",
    "\n",
    "**Example: Gender Classification**  \n",
    "\n",
    "Systems are often **black boxes**. Candidates might be sorted based on gender without knowing it.  \n",
    "This undermines autonomy and prevents contestation.\n",
    "\n",
    "**Example: Predictive Policing**  \n",
    "\n",
    "Police officers at times don’t even know how risk predictions are made.<sup>9</sup>  \n",
    "Lack of **explainability** and **accountability** limits meaningful oversight.\n",
    "\n",
    "Tools like **Model Cards**<sup>10</sup> and **Datasheets for Datasets**<sup>11</sup> help improve transparency but are rarely adopted unless legally required.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Sociotechnical and institutional constraints\n",
    "\n",
    "Fairness is not just a technical issue — it is embedded in **sociotechnical systems** shaped by organizations, stakeholders, and processes.<sup>5</sup>\n",
    "\n",
    "> A **sociotechnical system** refers to the interplay between **technical components** (e.g. models, data, software) and the **social structures** around them — including institutions, regulations and user behavior.  \n",
    "In such systems, technical decisions are never neutral. They are shaped by and impact human contexts.\n",
    "\n",
    "**Example: Predictive Policing**  \n",
    "\n",
    "Even a technically fair model still operates within **biased policing practices**.  \n",
    "ML systems can **legitimize unjust structures** if context is ignored.<sup>12</sup>\n",
    "\n",
    "**Example: Student Dropout Prediction** \n",
    "\n",
    "Universities may prioritize **cost-efficiency** over full support for all high-risk students.  \n",
    "Fairness interventions can clash with **institutional goals**.\n",
    "\n",
    "**Example: Gender Classification**  \n",
    "\n",
    "HR systems often rely on **third-party black-box models**.  \n",
    "Institutions may have no control over data or modeling decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### The Five Abstraction Traps<sup>5</sup>\n",
    "\n",
    "Fairness problems often result from **abstracting away social context**, to build generalizable, modular systems.  \n",
    "**Selbst et al.** (2019) describe five “abstraction traps” that lead to failed fairness efforts.\n",
    "\n",
    "| Trap | Description | Example |\n",
    "|:-----|:------------|:--------|\n",
    "| **Framing Trap** | Treating fairness as a modeling problem only | Ignoring why students drop out in the first place |\n",
    "| **Portability Trap** | Assuming fairness solutions are generalizable | Using standard metrics for gender classification |\n",
    "| **Formalism Trap** | Reducing fairness to math | Overemphasis on metric optimization (e.g. Equalized Odds) |\n",
    "| **Ripple Effect Trap** | Ignoring how ML systems change behavior | Predictive policing influences policing patterns |\n",
    "| **Solutionism Trap** | Assuming ML is always the best solution | Automating gender recognition without clear benefit |\n",
    "\n",
    "**Using the Traps as Reflective Questions**\n",
    "\n",
    "Selbst et al. suggest that these traps can be **turned into a critical checklist** by reversing their order and framing them as questions.  \n",
    "This helps structure ethical reflection in the development or assessment process.\n",
    "\n",
    "> **Start with the most fundamental question and work upward:**\n",
    "\n",
    "1. **Solutionism** → *Should we build this system at all? Is machine learning the right tool here?*  \n",
    "2. **Ripple Effect** → *How will the system change the environment it operates in?*  \n",
    "3. **Formalism** → *Which notions of fairness are relevant – and are they contested?*  \n",
    "4. **Portability** → *Does the fairness approach actually fit the specific context?*  \n",
    "5. **Framing** → *Are we addressing the right problem – or abstracting away key social dynamics?*\n",
    "\n",
    "> Applying fairness is not just about choosing a metric. It's about **asking the right questions**, in the right order.  \n",
    "Using these reversed abstraction traps as a guide helps ensure that fairness efforts are not only technical, but also socially meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications and Tools\n",
    "\n",
    "Improving fairness in machine learning requires more than technical interventions.  \n",
    "It involves institutional, procedural, and ethical considerations and faces **real obstacles** in practice.\n",
    "\n",
    "Despite growing awareness, many fairness recommendations are **not widely implemented**, unless supported by **regulatory enforcement**.  \n",
    "> Some companies adopt ethical guidelines mainly to **avoid binding oversight**, not out of intrinsic commitment.<sup>13</sup>\n",
    "\n",
    "In the following section, we highlight practical implications that can help support fairness.\n",
    "\n",
    "---\n",
    "\n",
    "### Fairness as a Process, Not a Fix\n",
    "\n",
    "Fairness is **not a static goal** or a property of the algorithm alone.  \n",
    "It is shaped by how the system is **designed, deployed, monitored** and situated within its **social context**.<sup>5</sup>\n",
    "\n",
    "This process-oriented view emphasizes:\n",
    "- **Continuous reflection** across the ML lifecycle\n",
    "- The need to assess **real-world consequences**\n",
    "- The importance of **stakeholder inclusion** and **interdisciplinary input**\n",
    "\n",
    "As discussed earlier, fairness metrics can be misleading when detached from impact.  \n",
    "> A **consequentialist perspective** asks: *Who benefits? Who is harmed? What actually changes for people involved?*\n",
    "\n",
    "The shift from solution-based to **process-based approaches** is supported by many authors. It shows that fairness is not a final state to be achieved, but more a commitment to **continuous reflection**.<sup>3,</sup><sup>5,</sup><sup>11,</sup><sup>14</sup>\n",
    "\n",
    "---\n",
    "\n",
    "### Fairness across the ML lifecycle\n",
    "\n",
    "Fairness interventions can happen at different stages:<sup>15</sup>\n",
    "\n",
    "- **Pre-processing:** Modify data before training (e.g. resampling, reweighting)\n",
    "- **In-processing:** Adjust algorithms (e.g. fairness constraints, adversarial learning)\n",
    "- **Post-processing:** Modify outputs after training (e.g. Fairlearn's ThresholdOptimizer)\n",
    "\n",
    "Each method requires choosing which type of fairness matters most and being aware of **contextual consequences**.\n",
    "\n",
    "---\n",
    "\n",
    "### Transparency, explainability, and accountability\n",
    "\n",
    "Improving fairness in machine learning is about making system **understandable, accessible, and contestable**.\n",
    "\n",
    "A key factor for this is **communication**:<sup>6,</sup><sup>7,</sup><sup>16</sup>\n",
    "> Fair ML systems must enable both **developers** and **affected individuals** to understand how decisions are made and to respond when they are unfair.\n",
    "\n",
    "#### Tools for transparency and documentation:\n",
    "\n",
    "- **Model Cards**:<sup>10</sup>\n",
    "   Document the model’s purpose, assumptions, limitations, and performance across demographic groups.  \n",
    "  → Should include **intended use**, **intersectional evaluation**, and **risk of misuse**.\n",
    "  \n",
    "- **Datasheets for Datasets**:<sup>11</sup>\n",
    "     Describe dataset origin, structure, collection methods, and potential biases.  \n",
    "  → Help evaluate whether a dataset is suitable for a given task or context.\n",
    "\n",
    "These tools increase visibility but they only help **if the information is accessible and communicated clearly**.\n",
    "\n",
    "#### Explainability<sup>7</sup>\n",
    "\n",
    "Even when models are documented, they may remain opaque.  \n",
    "**Explainability** means making the system’s behavior **understandable for humans**, especially those affected by its decisions.\n",
    "\n",
    "- For end users: explanations should be **simple and meaningful**\n",
    "- For domain experts: tools like **LIME** or **SHAP** can offer technical insights\n",
    "\n",
    "However, explainability often comes with trade-offs:\n",
    "- High-performing models may be hard to explain\n",
    "- Privacy concerns may limit what can be disclosed<sup>17</sup>\n",
    "\n",
    "#### Accountability\n",
    "\n",
    "Transparency and explainability are **necessary, but not sufficient**.  \n",
    "> Without **clear responsibility**, even a well-documented system lacks fairness.\n",
    "\n",
    "- Who decided which features to use?\n",
    "- Who defined the target variable?\n",
    "- Who can be contacted to contest a decision?\n",
    "\n",
    "Accountability requires **defined roles**, **oversight mechanisms**, and (ideally) the ability to **appeal or challenge** automated decisions.\n",
    "\n",
    "In all three case studies, affected individuals had little understanding of the system and no way to contest outcomes:\n",
    "- **Students** receiving dropout risk labels without explanation\n",
    "- **Job applicants** being filtered by black-box gender classifiers\n",
    "- **Citizens** being policed based on opaque hotspot predictions\n",
    "\n",
    "> Fairness depends on more than open data or transparent models.  \n",
    "It requires **communication structures** that enable understanding, trust, and recourse for **all stakeholders**, especially those affected by the system.<sup>4,</sup><sup>16</sup>\n",
    "\n",
    "---\n",
    "\n",
    "### Tools and Institutional Practices\n",
    "\n",
    "Several tools and practices can support fairness. However, these tools should not be used in isolation. They must be applied with an understanding of the social context and stakeholder needs.\n",
    "\n",
    "**Example Toolkits:**\n",
    "- **AIF360 (IBM)**:<sup>18</sup> Metrics and pre/in/post-processing methods for fairness evaluation\n",
    "- **Fairlearn (Microsoft)**:<sup>19</sup> Model diagnostics, fairness-accuracy trade-offs, and threshold tuning\n",
    "- **SageMaker Clarify (Amazon)**:<sup>20</sup> Bias detection and explainability tools\n",
    "\n",
    "**Institutional Practices:**\n",
    "\n",
    "Fairness in machine learning is not only a question of metrics or tools, it depends on **how organizations develop and govern these systems**.\n",
    "\n",
    "To move from abstract principles to meaningful practice, institutions can adopt several strategies:\n",
    "\n",
    "- Establish **fairness review boards**, ethics committees, or internal audit processes  \n",
    "  → These structures ensure that fairness decisions are **not left to individual developers**, but handled collectively and transparently.<sup>21</sup>\n",
    "\n",
    "- Provide **training on fairness, bias awareness, and societal impacts**<sup>16</sup>  \n",
    "  → Teams need to understand how their decisions affect real people — beyond just technical performance.\n",
    "\n",
    "- Promote **diverse team composition**<sup>22,</sup><sup>23</sup>  \n",
    "  → Homogeneous teams often fail to anticipate harms experienced by marginalized groups.  \n",
    "  **Example:** Lee (2018) shows that the lack of diversity in the tech industry can lead to serious blind spots. She describes a photo-tagging algorithm that labeled Black individuals as “gorillas”. This failure that could likely have been avoided with greater team diversity.\n",
    "\n",
    "- Enable **interdisciplinary collaboration**<sup>6</sup>  \n",
    "  → Fairness challenges require knowledge from law, sociology, ethics, and domain-specific fields.  \n",
    "  → Including these perspectives helps detect risks and design systems that are better aligned with societal values.\n",
    "\n",
    "- Involve **affected communities and domain experts** early in the process<sup>16,</sup><sup>24</sup>  \n",
    "  → Fairness cannot be defined solely by engineers.  \n",
    "  → Participatory design improves contextual understanding and **legitimizes fairness goals**.\n",
    "\n",
    "As the case studies illustrate, many harms arise not from malicious intent, but from **limited perspectives and missing expertise**.  \n",
    "> Addressing fairness requires institutions to invest in **inclusive practices, interdisciplinary thinking, and stakeholder engagement** — not just tools.\n",
    "\n",
    "---\n",
    "\n",
    "### Z-Inspection®: A practical fairness framework<sup>13,</sup><sup>25,</sup><sup>26</sup>\n",
    "\n",
    "Z-Inspection® offers a **holistic and interdisciplinary framework** to evaluate fairness in high-impact AI systems.\n",
    "\n",
    "\n",
    "| Phase | Description |\n",
    "|-------|-------------|\n",
    "| **Set-up** | Define scope and form independent, diverse expert team |\n",
    "| **Assess** | Develop sociotechnical scenarios, identify ethical tensions, map to EU AI guidelines |\n",
    "| **Resolve** | Generate concrete recommendations — including technical fixes or stopping deployment |\n",
    "\n",
    "![](Images/Z-Inspection.png)\n",
    "\n",
    "**Example:**  \n",
    "An AI system for detecting cardiac arrest in emergency calls showed reduced performance for non-native speakers.  \n",
    "Fairness concerns were not visible through metrics alone — they emerged through contextual assessment.\n",
    "\n",
    "> Z-Inspection® emphasizes that **fairness is embedded in systems, not just models**.  \n",
    "It requires **interdisciplinary collaboration**, **continuous monitoring**, and **ethical reflection** throughout the lifecycle.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Fairness is not a fixed goal** but a **continuous process** shaped by context and consequences.\n",
    "- Many standard fairness metrics have **mathematical and conceptual limitations**.\n",
    "- Each of the three case studies shows different challenges — and why **technical fixes alone aren’t enough**.\n",
    "- Addressing fairness means looking at the complete sociotechnical system and not just numbers.\n",
    "\n",
    "---\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "Hopefully this notebook has provided you with a solid foundation for understanding the **ethical challenges of fairness in machine learning**.\n",
    "The goal was not to present exhaustive solutions, but to introduce core concepts, raise awareness of common pitfalls, and encourage critical reflection.\n",
    "By combining theory, code, and case studies, the notebook offers an **entry point** into a complex and evolving field.\n",
    "Throughout the three integrated studies — on **student dropout prediction, gender classification**, and **predictive policing** – you have seen that fairness:\n",
    "\n",
    "- cannot be reduced to a single metric or technical fix\n",
    "- requires attention to representation, context, and long-term effects\n",
    "- must be assessed within the **broader sociotechnical environment** in which systems operate\n",
    "\n",
    "The notebook is designed to help you build a **conceptual framework** that you can expand over time — as technologies evolve, debates progress, and regulatory frameworks emerge.\n",
    "> **Note**: Legal and regulatory aspects (such as the EU AI Act) were not addressed in this notebook, but they are increasingly influencing real-world practice.\n",
    "\n",
    "\n",
    "If you are interested in learning more, here are some recommendations:\n",
    "\n",
    "| Source | Description | Link |\n",
    "|----------------|-------------|------|\n",
    "| Barocas et al. (2023) | Comprehensive overview of fairness, bias, and algorithmic decision-making | [Fair ML Book](https://fairmlbook.org) |\n",
    "| Verma & Rubin (2018) | Summary of different fairness metrics | [Fairness Metrics](https://doi.org/10.1145/3194770.3194776) |\n",
    "| Zicari et al. (2021) & Boonstra et al. (2024) | Real-world applications of the Z-Inspection® framework — shows what fairness assessments look like in practice | [AI in healthcare](https://doi.org/10.3389/fhumd.2021.673104) & [AI in nature monitoring](https://doi.org/10.48550/arXiv.2404.14366) |\n",
    "| Angwin et al. (2016) | Original investigation of the COMPAS case  | [COMPAS](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) |\n",
    "| Corbett-Davies (2023) | Highlights key challenges like pareto domination in fairness metrics | [Challenges in Fair ML](http://jmlr.org/papers/v24/22-1511.html.) |\n",
    "| Kaggle: Intro to AI Ethics | Practical exercises, interactive tutorials, and further links such as [Google's interactive explainer](https://research.google.com/bigpicture/attacking-discrimination-in-ml/) | [Kaggle course](https://www.kaggle.com/learn/intro-to-ai-ethics) |\n",
    "| Mehrabi et al. (2021) | A more compact overview of fairness and bias in ML (compared to Barocas et al.) | [Survey on Bias and Fairness](https://doi.org/10.1145/3457607) |\n",
    "| Lum & Isaac (2016) | Introduction to predictive policing | [Intro Predictive Policing](https://doi.org/10.1111/j.1740-9713.2016.00960.x) |\n",
    "| Robinson & Koepke (2016) | Builds on insights from Lum & Isaac | [More Predictive Policing](https://www.upturn.org/work/stuck-in-a-pattern) |\n",
    "\n",
    "Other references used in this notebook are listed in the user guide and cited throughout the sections.  \n",
    "For a deeper exploration, you can also access the master's thesis on which this notebook is based: *[GitHub](https://www.github.com/LukasWel/ethical-challenges-in-ml)*.\n",
    "\n",
    "Fairness in machine learning cannot be fully solved but it can be better **understood**, more **transparently discussed**, and more **responsibly addressed**.\n",
    "We hope this notebook helped you take the first step in that direction.\n",
    "\n",
    "---\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**1. True or False:**\n",
    "Fairness metrics can always tell us whether a system is fair in practice.\n",
    "\n",
    "1. [ ] True\n",
    "2. [ ] False\n",
    "\n",
    "**2. What does inframarginality describe?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] Errors that occur due to biased sampling\n",
    "2. [ ] Ignoring fairness concerns at the decision boundary\n",
    "3. [ ] When a fairness metric applies only to large groups\n",
    "4. [ ] When predictions are perfectly calibrated across all subgroups\n",
    "\n",
    "**3. What is a pareto-dominated outcome?**\n",
    "*(Select one option)*\n",
    "\n",
    "1. [ ] A system that performs equally across all metrics\n",
    "2. [ ] An outcome that benefits one group at the expense of another\n",
    "3. [ ] An outcome where a better alternative exists for all groups\n",
    "4. [ ] A fairness intervention that leads to identical thresholds for all groups\n",
    "   \n",
    "---\n",
    "\n",
    "#### Sources:\n",
    "1. Chouldechova, 2017\n",
    "2. Kleinberg et al., 2016\n",
    "3. Corbett-Davies et al., 2023\n",
    "4. Osoba & Welser, 2017\n",
    "5. Selbst et al., 2019\n",
    "6. Suresh & Guttag, 2021\n",
    "7. Ntoutsi et al., 2020\n",
    "8. Olteanu et al., 2019\n",
    "9. Robinson & Koepke, 2016\n",
    "10. Mitchell et al., 2019\n",
    "11. Gebru et al., 2018\n",
    "12. Lum & Isaac, 2016\n",
    "13. Boonstra et al., 2024\n",
    "14. Binns, 2018\n",
    "15. Calegari et al., 2023\n",
    "16. Barocas et al., 2023\n",
    "17. Schmidt et al., 2024\n",
    "18. LF AI & Data Foundation, n.d.\n",
    "19. Fairlearn Organization, n.d.\n",
    "20. Amazon Web Services, n.d.\n",
    "21. Raji et al., 2020\n",
    "22. Lee, 2018\n",
    "23. Yapo & Weiss, 2018\n",
    "24. Howard & Borenstein, 2018\n",
    "25. Zicari et al., 2021\n",
    "26. Zicari et al., 2022\n",
    "27. Scheuerman et al., 2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
